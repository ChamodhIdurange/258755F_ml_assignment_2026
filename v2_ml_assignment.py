# -*- coding: utf-8 -*-
"""V2_ML_Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mvXnRCMi2xco0ZTX6Z1-fjOgKdoUzMAT
"""

!pip install catboost shap lime matplotlib seaborn scikit-learn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import (classification_report, accuracy_score, f1_score,
                             roc_auc_score, confusion_matrix, roc_curve, auc)
from sklearn.preprocessing import LabelEncoder
import shap
import lime
import lime.lime_tabular
import pickle
import os

# =================================================================
# SECTION 1: DATA LOADING & PREPROCESSING (15 Marks)
# =================================================================
# Load the CSV exported from Microsoft Forms
# Ensure the filename matches what you uploaded
df_raw = pd.read_csv('survey_data.csv')

# Mapping long survey questions to technical feature names
column_mapping = {
    'Primary Department Question': 'Department',
    'Average Monthly Overtime': 'Overtime',
    'How many years has it been since your last job title change or promotion?': 'Promotion_Gap',
    'Satisfaction': 'Job_Satisfaction',
    'Risk': 'AI_Automation_Risk',
    'Has your specific department experienced staff layoffs or "firing" in the last 12 months?': 'Recent_Layoffs',
    'Security': 'Job_Security',
    'If you left today, how easy would it be to find a similar role elsewhere?': 'Market_Demand',
    'Are you actively planning to leave your current company or looking for a new job within the next 6 months?': 'Attrition'
}

df = df_raw.rename(columns=column_mapping)
df = df[list(column_mapping.values())]

# -----------------------------
# Data cleaning / normalization
# -----------------------------
# Normalize en-dash to hyphen in Overtime column (fixes character mismatch issue)
df['Overtime'] = df['Overtime'].astype(str).str.replace('â€“', '-', regex=False)

# Ensure numeric column is numeric (avoid poisoning numerics with 'Neutral')
df['Promotion_Gap'] = pd.to_numeric(df['Promotion_Gap'], errors='coerce')

# Data Cleaning: Convert Target to Binary (1 for Yes, 0 for No)
df['Attrition'] = df['Attrition'].map({'Yes': 1, 'No': 0}).astype('Int64')

# Fill missing categorical values only
cat_cols = ['Department', 'Overtime', 'Job_Satisfaction', 'AI_Automation_Risk',
            'Recent_Layoffs', 'Job_Security', 'Market_Demand']
df[cat_cols] = df[cat_cols].fillna('Neutral')

# Fill missing numeric values
df['Promotion_Gap'] = df['Promotion_Gap'].fillna(df['Promotion_Gap'].median())

# Finalize target dtype
df['Attrition'] = df['Attrition'].fillna(0).astype(int)

# Define categorical columns for the CatBoost algorithm
cat_features = ['Department', 'Overtime', 'Job_Satisfaction', 'AI_Automation_Risk',
                'Recent_Layoffs', 'Job_Security', 'Market_Demand']

# =================================================================
# SECTION 3: MODEL TRAINING & EVALUATION (20 Marks)
# =================================================================

# --- 3.1: Train / Validation / Test Split ---
# 80% for training+val, 20% for final testing
X = df.drop('Attrition', axis=1)
y = df['Attrition']
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Split the 80% again to get a 10% Validation set for Early Stopping
X_train, X_val, y_train, y_val = train_test_split(
    X_temp,
    y_temp,
    test_size=0.2,  # larger validation set to stabilize early stopping on small data
    random_state=42,
    stratify=y_temp,
)

print(f"Dataset Split Complete: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}")

# --- 3.2: Hyperparameter Choices ---
# Choosing CatBoost: handles categorical strings natively and prevents overfitting.
model = CatBoostClassifier(
    loss_function='Logloss',
    # Use Logloss for early stopping so training continues to improve probability quality
    # (AUC can hit 1.0 very early on small datasets, causing tiny models and ~0.5 outputs).
    eval_metric='Logloss',
    iterations=2000,          # Max number of trees
    learning_rate=0.05,       # Small steps for better accuracy
    depth=6,                  # Complexity of trees
    early_stopping_rounds=200, # Stop if validation score stops improving
    auto_class_weights='Balanced',
    random_seed=42,
    verbose=100               # Log progress every 100 iterations
)

# --- 3.3: Training ---
model.fit(X_train, y_train, cat_features=cat_features, eval_set=(X_val, y_val))

# Save model as pickle file for use in backend
os.makedirs('model', exist_ok=True)
model_data = {
    'model': model,
    'cat_features': cat_features,
    'feature_order': list(X.columns)
}
pickle_path = 'model/attrition_model.pkl'
with open(pickle_path, 'wb') as f:
    pickle.dump(model_data, f)
print(f"\nModel saved as pickle file: {pickle_path}")

# --- 3.4: Performance Metrics & Visuals ---
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

print("\n" + "="*30)
print("SECTION 3: PERFORMANCE RESULTS")
print("="*30)
print(classification_report(y_test, y_pred))
print(f"Overall Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"F1-Score:         {f1_score(y_test, y_pred):.4f}")
print(f"ROC-AUC Score:    {roc_auc_score(y_test, y_prob):.4f}")

# Plot 1: Confusion Matrix
plt.figure(figsize=(6, 4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix (Prediction vs Reality)')
plt.ylabel('Actual (0=Stay, 1=Leave)')
plt.xlabel('Predicted')
plt.show()

# Plot 2: ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_prob)
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, color='darkorange', label=f'ROC curve (area = {auc(fpr, tpr):.2f})')
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

# =================================================================
# SECTION 4: EXPLAINABILITY & INTERPRETATION (20 Marks)
# =================================================================

# --- 4.1: Global Explainability (SHAP) ---
print("\nGenerating SHAP Global Importance...")
explainer_shap = shap.TreeExplainer(model)
shap_values = explainer_shap.shap_values(X_test)
shap.summary_plot(shap_values, X_test) # Visualizes influential features

# --- 4.2: Local Explainability (LIME) ---
# Encode strings for LIME's internal math
X_train_lime = X_train.copy()
categorical_names = {}
encoders = {}
for i, col in enumerate(X.columns):
    if col in cat_features:
        le = LabelEncoder()
        X_train_lime[col] = le.fit_transform(X_train[col].astype(str))
        encoders[col] = le
        categorical_names[i] = le.classes_.tolist()

# Wrapper to turn LIME numbers back into strings for CatBoost
def predict_fn(data_as_np_array):
    temp_df = pd.DataFrame(data_as_np_array, columns=X.columns)
    for col in cat_features:
        temp_df[col] = encoders[col].inverse_transform(temp_df[col].astype(int))
    return model.predict_proba(temp_df)

explainer_lime = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train_lime.values,
    feature_names=X.columns.tolist(),
    categorical_features=[X.columns.get_loc(c) for c in cat_features],
    categorical_names=categorical_names,
    class_names=['Stay', 'Leave'],
    mode='classification'
)

# Explain Instance #0 for local interpretation
idx = 0
inst_encoded = X_test.iloc[idx].copy()
for col in cat_features:
    inst_encoded[col] = encoders[col].transform([inst_encoded[col]])[0]

exp = explainer_lime.explain_instance(inst_encoded.values.astype(float), predict_fn, num_features=5)
print(f"\nLIME Explanation for Case #{idx}:")
exp.show_in_notebook(show_table=True)